{
  final OFile file=fileSegment.files[0];
  int pos=0;
  lastOverflowBucket=file.readHeaderLong(0);
  pos+=OLongSerializer.LONG_SIZE;
  recordSplitPointer=file.readHeaderLong(pos);
  pos+=OLongSerializer.LONG_SIZE;
  roundCapacity=file.readHeaderLong(pos);
  pos+=OLongSerializer.LONG_SIZE;
  g=(int)file.readHeaderLong(pos);
  pos+=OLongSerializer.LONG_SIZE;
  d=(int)file.readHeaderLong(pos);
  pos+=OLongSerializer.LONG_SIZE;
  pageSize=file.readHeaderLong(pos);
  pos+=OLongSerializer.LONG_SIZE;
  nextPageSize=(int)file.readHeaderLong(pos);
  pos+=OLongSerializer.LONG_SIZE;
  size=file.readHeaderLong(pos);
  pos+=OLongSerializer.LONG_SIZE;
  mainBucketsSize=file.readHeaderLong(pos);
  final OFile statisticsFile=overflowStatistic.file;
  pos=0;
  final int serializedBitSetSize=statisticsFile.readInt(pos);
  pos+=OIntegerSerializer.INT_SIZE;
  byte[] serializedBitSet=new byte[serializedBitSetSize];
  statisticsFile.read(pos,serializedBitSet,serializedBitSetSize);
  OMemoryInputStream byteArrayInputStream=new OMemoryInputStream(serializedBitSet);
  ObjectInputStream objectInputStream=new ObjectInputStream(byteArrayInputStream);
  try {
    splittedBuckets=(BitSet)objectInputStream.readObject();
  }
 catch (  ClassNotFoundException e) {
    throw new OStorageException("Error during opening cluster " + name,e);
  }
  pos+=serializedBitSet.length;
  objectInputStream.close();
  objectInputStream=null;
  byteArrayInputStream=null;
  serializedBitSet=null;
  final int mapSize=statisticsFile.readInt(pos);
  pos+=OIntegerSerializer.INT_SIZE;
  mainBucketOverflowInfoByIndex=new HashMap<Long,Integer>(mapSize);
  for (int i=0; i < mapSize; i++) {
    final long key=statisticsFile.readLong(pos);
    pos+=OLongSerializer.LONG_SIZE;
    final int value=statisticsFile.readInt(pos);
    pos+=OIntegerSerializer.INT_SIZE;
    mainBucketOverflowInfoByIndex.put(key,value);
  }
  rebuildGroupOverflowChain();
}
