{
  byte operation=-1;
  final ORecordId rid=(ORecordId)iRecord.getIdentity();
switch (iType) {
case AFTER_CREATE:
    operation=OReplicationTask.CREATE;
  break;
case AFTER_UPDATE:
operation=OReplicationTask.UPDATE;
break;
case AFTER_DELETE:
operation=OReplicationTask.DELETE;
break;
}
if (operation > -1) {
final String clusterName=storage.getClusterById(rid.getClusterId()).getName();
if (!canExecuteOperation(clusterName,operation,"out")) {
OLogManager.instance().debug(this,"DISTRIBUTED -> skip sending operation %s against cluster '%s' to remote nodes because of the distributed configuration",OReplicationTask.getName(operation).toUpperCase(),clusterName);
return false;
}
final EXECUTION_MODE mode=getOperationMode(clusterName,operation);
for (OReplicationLog localLog : logs.values()) {
try {
if (localLog.appendLog(operation,rid,iRecord.getVersion()) == -1) {
OLogManager.instance().warn(this,"DISTRIBUTED -> replication log limit reached for file: %s. The node will be offline and a manual alignment will be needed",localLog);
}
}
 catch (IOException e) {
OLogManager.instance().error(this,"DISTRIBUTED -> Error on appending log in file: %s. The coherence of the cluster is not more guaranteed",e,localLog);
}
}
final Set<String> members=cluster.getRemoteNodeIds();
if (!members.isEmpty()) {
final Collection<Object> results=cluster.executeOperation(members,operation,storageName,rid,iRecord.getVersion(),new ORawBuffer((ORecordInternal<?>)iRecord),mode);
for (String member : members) {
final OReplicationLog log=getLog(member);
try {
log.success();
}
 catch (IOException e) {
OLogManager.instance().error(this,"DISTRIBUTED -> Error on reset log file: %s",e,log);
}
}
}
}
return false;
}
